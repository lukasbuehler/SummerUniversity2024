{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the performance of the vecadd kernel\n",
    "\n",
    "\n",
    "In this section we will see how we can avoid the JIT cost of Numba, how we can measure the performance of the kernel without the `%timeit` magic, how we can use `nvprof`, the CUDA profiler to analyze the performance of the kernel, and finally, we will evaluate the performance of the kernel.\n",
    "\n",
    "## Avoiding the JIT cost\n",
    "\n",
    "The previous exercise has shown that Numba will compile the CUDA kernel every time we call our program and, in order to amortize the compilation cost, we need several invocations. We would like to avoid this cost.\n",
    "\n",
    "Unlike the `@numba.jit` decorator, `@cuda.jit` does not accept a `cache` parameter, that would cache the generated code on the disk and use it on subsequent invocations of the program. Nonetheless, we can force the code generation at import time by supplying a function signature to the `@cuda.jit` decorator that describes the CUDA kernel. This will generate the CUDA code at the time when the decorator processes the function declaration and, therefore, we will avoid the runtime cost of JIT. Let's see how this is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba.cuda as cuda\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#@cuda.jit('void(Array(float64, 1, \"C\"), Array(float64, 1, \"C\"), Array(float64, 1, \"C\"))')\n",
    "@cuda.jit('void(float64[::1], float64[::1], float64[::1])')\n",
    "def _vecadd_cuda(z, x, y):\n",
    "    '''The CUDA kernel'''\n",
    "    i = cuda.grid(1)\n",
    "    N = x.shape[0]\n",
    "    if i >= N:\n",
    "        return\n",
    "\n",
    "    z[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This instructs the Numba runtime to compile the following function into a CUDA kernel (return type `void`) accepting three one-dimensional arrays of `float64` (or `double`) stored in row-major order (C convention). This way, Numba does not have to wait until the `_vecadd_cuda` function is called to figure out the argument types and compile the kernel. It can do this at import time, when it first encounters the function. The downside to that is that you can't call the function with a different type of arguments later. For more details on how you can specify function signatures in Numba, see [here](http://numba.pydata.org/numba-doc/latest/reference/types.html#numba-types).\n",
    "\n",
    "Let's retry our example now with this version of the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the random engine\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "%timeit -r2 -n 4 _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "> Time the kernel with `%timeit -n1 -r1`. Try to increase the repetitions and experiment with different array sizes. What do you see?\n",
    "\n",
    "## Measuring the execution time of the kernel\n",
    "\n",
    "All you see from the previous exercise is the same execution time! What is happening? Actually, you are not measuring the kernel execution time, but rather the kernel launch time. CUDA kernels are launched asynchronously. This means that as soon as you launch the kernel on the GPU, the CPU will continue execution. In this case, it will continue executing and it will block at the statement that copies back the result to the host. \n",
    "\n",
    "How do we measure the kernel execution time then? For this, we are going to write a Python [context manager](https://docs.python.org/3.8/reference/datamodel.html?highlight=__getitem__#with-statement-context-managers) so as to measure the execution time of a region in a nice, Pythonic way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class time_region:\n",
    "    def __init__(self, time_offset=0):\n",
    "        self._time_off = time_offset\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start = time.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end = time.time()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + (self._t_end - self._t_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details about context managers, please refer elsewhere. Let's use our timer to time the kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "with time_region() as t_kernel:\n",
    "    _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "print(f'CUDA kernel time: {t_kernel.elapsed_time()} s')\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our timer seems to work fine; we still measure the kernel launch time as with `%timeit`. In order to measure the actual kernel execution time, we need to block the CPU calling thread until the kernel finishes, immediately after we launch the kernel. We can achieve that with `cuda.synchronize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "with time_region() as t_kernel:\n",
    "    _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "    cuda.synchronize()\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    z = x + y\n",
    "\n",
    "print(f'CUDA kernel time: {t_kernel.elapsed_time()} s')\n",
    "print(f'Numpy time:       {t_ref.elapsed_time()} s')\n",
    "\n",
    "\n",
    "# Copy back the result to the host\n",
    "res = d_z.copy_to_host()\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, the CUDA kernel is 15x faster than the native Numpy kernel.\n",
    "\n",
    "Before analysing how good or bad this is, let's see an alternative way for measuring the kernel time that actually avoids the use of `cuda.synchronize()`.\n",
    "\n",
    "## Measuring the kernel execution time with CUDA events\n",
    "\n",
    "Inserting `cuda.synchronize()` without a reason could slow down your application, since it not only blocks the current CPU thread, but also imposes a synchronization point for all the CUDA streams on the GPU that are currently running in parallel.\n",
    "\n",
    "> A CUDA stream is essentially a series of sequential operations (data transfers, kernel launches, etc.) that execute on the GPU. Multiple CUDA streams may run independently on the GPU, thus allowing overlapping of operations, such as data transfers and execution of kernels.\n",
    "\n",
    "To avoid this, but also to obtain a more precise measurement, you can use [CUDA events](http://docs.nvidia.com/cuda/cuda-c-programming-guide/#events). You can imagine CUDA events as milestones associated with timestamps that you can insert between operations in a CUDA stream. Let's how we can adapt our `time_region` context manager to use CUDA events:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class time_region_cuda:\n",
    "    def __init__(self, time_offset=0, cuda_stream=0):\n",
    "        self._t_start = cuda.event(timing=True)\n",
    "        self._t_end = cuda.event(timing=True)\n",
    "        self._time_off = time_offset\n",
    "        self._cuda_stream = cuda_stream\n",
    "\n",
    "    def __enter__(self):\n",
    "        self._t_start.record(self._cuda_stream)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self._t_end.record(self._cuda_stream)\n",
    "        self._t_end.synchronize()\n",
    "\n",
    "    def elapsed_time(self):\n",
    "        return self._time_off + 1.e-3*cuda.event_elapsed_time(self._t_start, self._t_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure a data region with CUDA events you first need to create two events: one for the start and one for the end. You can achieve that with the `cuda.event(timing=True)`. To start counting, you need to call `record()` on the starting event marking the \"arrival\" to that milestone. Similarly, you call `record()` on the ending event to mark the end of the region. Then you can obtain the elapsed time using the corresponding function as shown in the example above.\n",
    "\n",
    "Let's rewrite our vector addition example using the CUDA event timers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = cuda.pinned_array(N)\n",
    "y = cuda.pinned_array(N)\n",
    "rng.random(N, out=x)\n",
    "rng.random(N, out=y)\n",
    "z = cuda.pinned_array(N)\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "d_x = cuda.to_device(x)\n",
    "d_y = cuda.to_device(y)\n",
    "d_z = cuda.to_device(z)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "with time_region_cuda() as t_kernel:\n",
    "    _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    z = x + y\n",
    "    \n",
    "with time_region_cuda() as t_copyout:\n",
    "    d_z.copy_to_host(\n",
    "        \n",
    "outrint(f'CUDA kernel time: {t_kernel.elapsed_time()} s')\n",
    "print(f'Numpy time:       {t_ref.elapsed_time()} s')\n",
    "\n",
    "\n",
    "# Copy back the result to the host\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the execution time obtained is the correct one without having to use `cuda.synchronize()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the performance of the kernel\n",
    "\n",
    "The question that arises is how good is the performance that we achieve. Let's inspect further the kernel. Each thread does two `float64` reads from the memory and one write and performs an addition. That means for one floating operation, the kernel must transfer to/from memory 24 bytes from the main memory. This gives us an  *arithmetic intensity* or *flop:byte ratio* of 0.0417. The lower this ratio is for a computational kernel, the more likely will be that the kernel is memory bandwidth bound. As the ratio increases, the kernel tends to be more compute bound. The theory behind the arithmetic intensity is covered by the *Roofline* performance model, which is outside the scope of this tutorial. For the moment, let's compute two performance metrics, the `Gflop/s` achieved by the kernel and the data transfer rate to/from memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Performance: {1.e-9*N/t_kernel.elapsed_time()} Gflop/s')\n",
    "print(f'Transfer rate: {1.e-9*3*N*8/t_kernel.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the same for the NumPy kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Performance: {1.e-9*N/t_ref.elapsed_time()} Gflop/s')\n",
    "print(f'Transfer rate: {1.e-9*3*N*8/t_ref.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can, the GPU can deliver more than 10x bandwidth compared to the CPU. Checking the [datasheet](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/tesla-p100/pdf/nvidia-tesla-p100-datasheet.pdf) of the NVIDIA P100 GPU, we can see that the peak nominal memory bandwidth is 732 GB/s, meaning that our kernel utilizes 70% of the peak bandwidth.\n",
    "\n",
    "> Achieving the nominal peak memory bandwidth is usually not possible with real-life computational kernels, even with very low arithmetic intensity. For this reason, we tend to cite the *effective memory bandwidth*, which is obtained by benchmarks like the one presented in this tutorial. In fact, the effective memory bandwidth of the P100 GPUs is at ~550 GB/s, which essentially shows that the vector addition kernel's performance is optimal. For the Haswell CPUs on the host, the effective memory bandwidth is ~50 GB/s.\n",
    "\n",
    "> NOTE: The numpy vector addition performance is not ideal, since it can't reach the memory bandwidth limit as it ought to. The problem could be related to CPU affinity issues, but we are not going to address them in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Understanding the data transfer overhead\n",
    "\n",
    "So far we have only focused on the performance of the kernel. There is still a quite important topic we have not yet addressed. CUDA kernels require that the data they operate on is located on the device and we need to move that data there from the host. What is the cost of this data movement? Let's time our benchmark code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the host vectors\n",
    "N = 200*1000*1000\n",
    "x = rng.random(N)\n",
    "y = rng.random(N)\n",
    "\n",
    "\n",
    "# Copy and allocate data on the device\n",
    "with time_region_cuda() as t_copyin:\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_y = cuda.to_device(y)\n",
    "\n",
    "with time_region_cuda() as t_create:\n",
    "    d_z = cuda.device_array_like(x)\n",
    "\n",
    "# Set up the kernel invocation\n",
    "block_size = 128\n",
    "num_blocks = N // block_size\n",
    "if N % block_size:\n",
    "    num_blocks += 1\n",
    "\n",
    "# Call the kernel\n",
    "with time_region_cuda() as t_kernel:\n",
    "    _vecadd_cuda[num_blocks, block_size](d_z, d_x, d_y)\n",
    "\n",
    "with time_region() as t_ref:\n",
    "    z = x + y\n",
    "\n",
    "print(f'CUDA kernel time: {t_kernel.elapsed_time()} s')\n",
    "print(f'Numpy time:       {t_ref.elapsed_time()} s')\n",
    "\n",
    "\n",
    "# Copy back the result to the host\n",
    "with time_region_cuda() as t_copyout:\n",
    "    d_z.copy_to_host(res)\n",
    "\n",
    "print(f'Copyin time:  {t_copyin.elapsed_time()} s')  \n",
    "print(f'Create time:  {t_create.elapsed_time()} s')    \n",
    "print(f'Copyout time: {t_copyout.elapsed_time()} s')    \n",
    "\n",
    "\n",
    "# Validate the result\n",
    "assert np.allclose(x + y, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data copy times are quite important! In fact, if we include these in the total execution time of the GPU version, the CPU version becomes more than 8x faster! Data transfers is the No. 1 optimization that you should do when programming for the GPUs. You must minimize the data transfers to/from GPU by keeping the necessary data on the GPU for as long as it is needed.\n",
    "\n",
    "Before closing this discussion, let's see how fast is the data moved over to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Copyin rate: {1e-9*2*N*8/t_copyin.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is bound by the data rate of the PCI 16x bus where the GPU is attached to and, it is indeed way too slower than the main memory bandwidth of modern processors.\n",
    "\n",
    "Interestingly, the copyout data rate seems to be much slower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Copyout rate: {1e-9*N*8/t_copyout.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, memory allocated on the host is pageable. This means that it can be moved by the OS kernel to a secondary storage device if there is not enough memory available on the system. This can incur a significant performance penalty, especially if you write on freshly allocated memory (as it happens in our example). You can avoid this overhead by using *page-locked* or *pinned* memory. This memory cannot be paged out and it is physically resident on the memory device. CUDA gives you the opportunity to use pinned memory and Numba allows you to create pinned ndarrays using the [cuda.pinned_array()](http://numba.pydata.org/numba-doc/latest/cuda-reference/memory.html#numba.cuda.pinned_array) function.\n",
    "\n",
    "> In order to keep track of which memory pages are resident on the physical memory and which are not, the OS kernel maintains a special data structure called *page table*. When you allocate memory on the host, the OS kernel simply creates a virtual memory mapping and it does not allocate any physical page. As soon as you start writing to the memory area you have been allocated, it will look for the page in its page tables and if not found, a *page fault* will be raised and then the kernel will have to physically allocate the missing memory page and update its page tables.\n",
    "\n",
    "Let's rewrite the copyout part using pinned memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cuda.pinned_array(N)\n",
    "with time_region_cuda() as t_pinned:\n",
    "    d_z.copy_to_host(res)\n",
    "    \n",
    "assert np.allclose(x + y, res)\n",
    "print(f'Copyout data rate (pinned): {1e-9*N*8/t_pinned.elapsed_time()} GB/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how much the performance has improved. It is now even better than the copyin operation.\n",
    "\n",
    "However, pinned memory does not come without a cost. Since pinned pages cannot be paged out, they will stay on the physical memory, increasing the memory pressure and, finally, the effective memory consumption of the code. For memory hungry applications, this can be a problem.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "> Apply the pinned memory technique also to the input arrays `x` and `y`.\n",
    "\n",
    "## Profiling the CUDA code (optional)\n",
    "\n",
    "In this simple example of vector addition we assessed the performance and identified the bottlenecks ourselves, by analyzing the code structure and reasoning about it. In more complex codes or codes that you are not very familiar with, it would be good if this analysis could be done by a dedicated tool. Not to be misunderstood, understanding the code structure and its memory and compute requirements is essential for optimizing it in any case, but using a *performance profiler* is very handy for analyzing the performance bottlenecks, for helping you prioritizing your optimization targets and for understanding how much room for improvement exists.\n",
    "\n",
    "NVIDIA provides [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) for profiling the code and inspecting the results as well as the older [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview) and [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual).\n",
    "\n",
    "> You may install the Nvidia Nsights Systems and the Nvidia Visual Profiler on your personal computer and visualize the performance results, even if you don't have a GPU.\n",
    "\n",
    "The `numba-cuda/src/vecadd.py` file contains the vector addition example as we have finally presented it here. Let's do a basic profing.\n",
    "\n",
    "First, we need to open a new terminal (\"File->New->Terminal\") and activate the virtual environment of the course's Python kernel.\n",
    "\n",
    "Then we need to load the Nsight profiler:\n",
    "\n",
    "```bash\n",
    "module load daint-gpu\n",
    "module load nvidia-nsight-systems\n",
    "```\n",
    "\n",
    "Now it's time to do our basic profiling:\n",
    "\n",
    "```bash\n",
    "cd PythonHPC/numba-cuda\n",
    "nsys profile -o vecadd.qdrep python3 src/vecadd.py $((200*1000*1000))\n",
    "```\n",
    "\n",
    "> If you want to profile your code with `nvprof` you should call `cuda.profile_stop()` at the end of your program.\n",
    "\n",
    "This profiling provides basic information about the data transfers and the execution time of the different kernels. It is the first step you need to take, because this will show you how much time you spend on transferring data to/from the device and which kernels are the most time consuming. Here is a screenshot for our example:\n",
    "\n",
    "![Profiling of the vector addition benchmark](figs/vecadd-nsight.png)\n",
    "\n",
    "Notice how much is the overhead of the data transfers as well as that of the pinned memory allocation of the `res` array. Placing your cursor on top of any of the regions in the timeline you can see more information. In this case, I have highlighted the copy-to-host operation, where you can see that the target memory on the host is pinned and the data rate is exactly as the one we have calculated above.\n",
    "\n",
    "As soon as you have addressed the data transfer issues, the next step is to identify the performance bottlenecks in the most time consuming kernels. To do that, you need more detailed information that reflects hardware events happening on the GPU (e.g., instructions executed, data transferred from the main memory, use of caches etc.). To achieve this we will need to use `nvprof` as follows:\n",
    "\n",
    "```bash\n",
    "module load cudatoolkit\n",
    "nvprof -o vecadd.detailed.nvprof --analysis-metrics python src/vecadd.py $((200*1000*1000))\n",
    "```\n",
    "\n",
    "> NOTE: From Volta GPUs onward, you are advised to use the [Nvidia Nsight Compute](https://developer.nvidia.com/nsight-compute) tool.\n",
    "\n",
    "As you will notice this command incurs quite of an overhead. The reason behind that is that there are only a few hardware performance monitoring registers on the GPU, so in order for `nvprof` to collect all the necessary performance metrics, it will have to rerun the kernel several times.\n",
    "\n",
    "From this type of analysis you can obtain the actual memory bandwidth consumed by your kernel and this is what shows up for our vector addition kernel:\n",
    "\n",
    "![Detailed profiling of the vector addition benchmark](figs/vecadd-detailed-nvprof.png)\n",
    "\n",
    "As expected the memory bandwidth consumption is red highlighted since it is the performance limiting factor, as we have also calculated manually. The bandwidth consumption reported here is 557 GB/s, which is quite close with our measurements based on the algorithm details.\n",
    "\n",
    "This concludes our discussion on measuring and analyzing the performance of a CUDA program written with Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpcpy2023",
   "language": "python",
   "name": "hpcpy2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
