{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Computing with [CuPy](https://cupy.chainer.org/)\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/cupy/cupy/main/docs/image/cupy_logo_1000px.png\" width=\"400\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy in a Nutshell\n",
    "\n",
    "* CuPy is a matrix library accelerated with [NVIDIA CUDA](https://developer.nvidia.com/cuda-zone). Using [CUDA-related GPU-accelerated libraries](https://developer.nvidia.com/gpu-accelerated-libraries) (cuBLAS, cuDNN, cuRand, cuSolver, cuSPARSE, cuFFT, NCCL) it allows to take full advantage of the computing power of GPUs.\n",
    "* CuPy's core component is the `cupy.ndarray` class which is highly compatible with `numpy.ndarray`.\n",
    "* It supports most of the high level operations of `NumPy` arrays and allows to write user defined kernels to execute on the GPU.\n",
    "* CuPy produces GPU kernels optimized for the shapes and dtypes of given arguments on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timers import cupy_timer, cpu_timer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating CuPy arrays directly on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = cp.zeros((1000, 1000))\n",
    "print(f'Array A is of type: {type(A)}')\n",
    "\n",
    "B = cp.array([[1, 2, 3], \n",
    "              [4, 5, 6]])\n",
    "\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_rng = cp.random.default_rng()\n",
    "B = cp_rng.random((1000, 1000))\n",
    "mu = B.mean()\n",
    "print(f'The mean value is: {mu}')\n",
    "print(f'The type of mu is: {type(mu)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Question</mark>: What is the type of the elements of array `B`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transferring Arrays from Host to Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_rng = np.random.default_rng()\n",
    "A_cpu = np_rng.random((1000, 1000))\n",
    "\n",
    "# Create CuPy array from NumPy array\n",
    "A_gpu = cp.array(A_cpu)\n",
    "\n",
    "mu_cpu = A_cpu.mean()\n",
    "mu_gpu = A_gpu.mean()\n",
    "print(f'Mean using CPU: {mu_cpu}')\n",
    "print(f'Mean using GPU: {mu_gpu}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranferring CuPy arrays to Host\n",
    "\n",
    "Tranferring of CuPy arrays to host is performed using the `cupy.ndarrray.get` method or the `cupy.asnumpy` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_gpu = cp.ones((1000, 1000))\n",
    "x_cpu = x_gpu.get()\n",
    "print(f'Type of CPU array is: {type(x_cpu)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gpu = cp_rng.random((1000, 1000))\n",
    "y_cpu = cp.asnumpy(y_gpu)\n",
    "print(f'Type of CPU array is: {type(y_cpu)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical CuPy Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np_rng.random((2000, 2000))\n",
    "y_cpu = np_rng.random((2000, 2000))\n",
    "x_gpu = cp.array(x_cpu)\n",
    "y_gpu = cp.array(y_cpu)\n",
    "\n",
    "z_cpu = x_cpu @ y_cpu\n",
    "z_gpu = x_gpu @ y_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question: Time each matrix multiplication (CPU vs GPU). Try with different array sizes. What do you notice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear System Solving NumPy vs CuPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_cpu = np_rng.random((10000, 10000))\n",
    "b_cpu = np_rng.random(10000)\n",
    "A_gpu = cp.array(A_cpu)\n",
    "b_gpu = cp.array(b_cpu)\n",
    "\n",
    "with cpu_timer():\n",
    "    np.linalg.solve(A_cpu, b_cpu)\n",
    "    \n",
    "with cupy_timer():\n",
    "    cp.linalg.solve(A_gpu, b_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean distance matrix\n",
    "\n",
    "$\n",
    "    d_e(\\mathbf x, \\mathbf y) =\n",
    "    \\begin{bmatrix}\n",
    "    \\sum_{i=1}^n (x_{1i}-y_{1i})^2 & \\sum_{i=1}^n(x_{1i}-y_{2i})^2 & \\cdots & \\sum_{i=1}^n (x_{1i}-y_{ni})^2 \\\\  \n",
    "    \\sum_{i=1}^n(x_{2i}-y_{1i})^2 & \\sum_{i=1}^n(x_{2i}-y_{2i})^2 & \\cdots & \\sum_{i=1}^n(x_{2i}-y_{ni})^2 \\\\  \n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\sum_{i=1}^n(x_{ni}-y_{1i})^2 & \\sum_{i=1}^n(x_{ni}-y_{2i})^2 & \\cdots & \\sum_{i=1}^n(x_{ni}-y_{ni})^2 \\\\  \n",
    "    \\end{bmatrix}\n",
    "$\n",
    "\n",
    "## Vectorization friendly summation \n",
    "$ \n",
    "\\sum_{k=1}^n \\left(x_{ik}-y_{jk}\\right)^2 = \\left(\\vec{x_i} - \\vec {y_j}\\right)\\cdot \\left(\\vec{x_i} - \\vec{y_j}\\right)=\\vec{x_i} \\cdot \\vec{x_i} + \\vec{y_j} \\cdot \\vec{y_j} -2\\vec{x_i}\\cdot \\vec{y_j}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_cpu(x, y):\n",
    "    x2 = np.einsum('ij,ij->i', x, x)[:, np.newaxis]\n",
    "    y2 = np.einsum('ij,ij->i', y, y)[np.newaxis, :]\n",
    "    xy = x @ y.T\n",
    "    return np.abs(x2 + y2 - 2.0 * xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance_gpu(x, y):\n",
    "    x2 = cp.einsum('ij,ij->i', x, x)[:, cp.newaxis]\n",
    "    y2 = cp.einsum('ij,ij->i', y, y)[cp.newaxis, :]\n",
    "    xy = x @ y.T\n",
    "    return cp.abs(x2 + y2 - 2.0 * xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np_rng.random((5000, 5000))\n",
    "y_cpu = np_rng.random((5000, 5000))\n",
    "x_gpu = cp.array(x_cpu)\n",
    "y_gpu = cp.array(y_cpu)\n",
    "\n",
    "with cpu_timer():\n",
    "    eu_cpu = euclidean_distance_cpu(x_cpu, y_cpu)\n",
    "\n",
    "with cupy_timer():\n",
    "    eu_gpu = euclidean_distance_gpu(x_gpu, y_gpu)\n",
    "    \n",
    "assert(np.allclose(eu_cpu, eu_gpu.get()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make function work for both CuPy/NumPy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y):\n",
    "    array_mod = cp.get_array_module(x)\n",
    "    x2 = array_mod.einsum('ij,ij->i', x, x)[:, array_mod.newaxis]\n",
    "    y2 = array_mod.einsum('ij,ij->i', y, y)[array_mod.newaxis, :]\n",
    "    xy = x @ y.T\n",
    "    return array_mod.abs(x2 + y2 - 2.0 * xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cpu_timer():\n",
    "    eu_cpu = euclidean_distance(x_cpu, y_cpu)\n",
    "\n",
    "with cupy_timer():\n",
    "    eu_gpu = euclidean_distance(x_gpu, y_gpu)\n",
    "    \n",
    "assert(np.allclose(eu_cpu, eu_gpu.get()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating/Plotting [Julia Sets](https://en.wikipedia.org/wiki/Julia_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import prange, njit\n",
    "\n",
    "@njit(parallel=True)\n",
    "def julia_set_numba(X, Y, cx, cy, iter_max, radius2):\n",
    "    ny = X.shape[0]\n",
    "    nx = Y.shape[1]\n",
    "    julia = np.zeros_like(X, dtype=np.int32)\n",
    "    for i in prange(ny):\n",
    "        for j in range(nx):\n",
    "            x = X[i, j]\n",
    "            y = Y[i, j]\n",
    "            k = 0\n",
    "            while x * x + y * y < radius2 and k < iter_max:\n",
    "                x, y = x * x - y * y + cx, 2.0 * x * y + cy\n",
    "                k = k + 1\n",
    "                \n",
    "            julia[i, j] = k \n",
    "            \n",
    "    return julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(np.linspace(-2.0 , 2.0, 5000), np.linspace(-2.0, 2.0, 5000))\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "c = -0.9 + 0.22143j\n",
    "radius2 = 4.0\n",
    "with cpu_timer():\n",
    "    julia = julia_set_numba(X, Y, c.real, c.imag, 100, radius2)\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.imshow(julia, extent=[-2, 2, -2, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elementwise Kernel\n",
    "\n",
    "Using the `cupy.ElementwiseKernel` class it is easy to create GPU kernels by defining the computation that is going to be applied to each element of the input array(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_kernel = cp.ElementwiseKernel('float64 X, float64 Y, float64 cx, float64 cy, int32 itermax, float64 radius2',\n",
    "                                    'int32 julia',\n",
    "                                    f'''julia = 0;\n",
    "                                    double x = X, y = Y;\n",
    "                                    double xtemp;\n",
    "                                    int nit = 0;\n",
    "                                    while(x * x + y * y < radius2 && nit < itermax) {{\n",
    "                                        xtemp = x * x - y * y + cx;\n",
    "                                        y = 2.0 * x * y + cy;\n",
    "                                        x = xtemp;\n",
    "                                        nit += 1;\n",
    "                                    }}\n",
    "                                    julia = nit;''', 'julia_kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = cp.meshgrid(cp.linspace(-2.0 , 2.0, 5000), cp.linspace(-2.0, 2.0, 5000))\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "c = -0.9 + 0.22143j\n",
    "with cupy_timer():\n",
    "    julia_gpu = julia_kernel(X, Y, c.real, c.imag, 100, 4.0)\n",
    "    julia_array = julia_gpu.get()\n",
    "ax.set_aspect('equal')\n",
    "ax.imshow(julia_array, extent=[-2, 2, -2, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Kernel\n",
    "\n",
    "It is possible to create raw CUDA kernels using the `cupy.RawKernel` class but the block and grid dimensions are not handled automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julia_rawkernel = cp.RawKernel(r'''\n",
    "        extern \"C\" \n",
    "        __global__ void julia_rawkernel(const double* X, const double* Y, const double cx,\n",
    "                              const double cy, const int itermax, const int nx,\n",
    "                              const int ny, const double radius2, int* julia)  {\n",
    "        int tidx = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        int tidy = blockDim.y * blockIdx.y + threadIdx.y;\n",
    "        int niter = 0;\n",
    "        double tempx;\n",
    "        if(tidx < nx && tidy < ny) \n",
    "        {\n",
    "            int tid = tidy * nx + tidx;\n",
    "            double x = X[tid], y = Y[tid];\n",
    "            while((x * x + y * y) < radius2 && niter < itermax) {\n",
    "                tempx = x * x - y * y + cx;\n",
    "                y = 2.0 * x * y + cy;\n",
    "                x = tempx;\n",
    "                niter +=1 ;\n",
    "            }\n",
    "            julia[tid] = niter;\n",
    "        }\n",
    "    }''', 'julia_rawkernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "c = -0.9 + 0.22143j\n",
    "threads_x = 32\n",
    "threads_y = 32\n",
    "x_points = 5000\n",
    "y_points = 5000\n",
    "x_blocks = (x_points - 1) // threads_x + 1 \n",
    "y_blocks = (y_points - 1) // threads_y + 1\n",
    "X_gpu, Y_gpu = cp.meshgrid(cp.linspace(-2.0 , 2.0, x_points), cp.linspace(-2.0, 2.0, y_points))\n",
    "with cupy_timer():\n",
    "    julia_gpu = cp.zeros_like(X_gpu, dtype=cp.int32)\n",
    "    julia_rawkernel((x_blocks, y_blocks), (threads_x, threads_y), (X_gpu, Y_gpu, c.real, c.imag, 100, \n",
    "                    x_points, y_points, 2.0 ** 2, julia_gpu))\n",
    "    julia_array = julia_gpu.get()\n",
    "ax.imshow(julia_array, extent=[-2, 2, -2, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Kernel JIT (experimental)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cupyx import jit\n",
    "\n",
    "@jit.rawkernel()\n",
    "def julia_kernel(X, Y, cx, cy, itermax, nx, ny, radius2, julia):\n",
    "    tidx = jit.blockDim.x * jit.blockIdx.x + jit.threadIdx.x\n",
    "    tidy = jit.blockDim.y * jit.blockIdx.y + jit.threadIdx.y\n",
    "    x = X[tidy, tidx]\n",
    "    y = Y[tidy, tidx]   \n",
    "    niter = 0\n",
    "    if tidx < nx and tidy < ny:\n",
    "        while (x * x + y * y) < radius2 and niter < itermax:\n",
    "            x, y = (x * x - y * y + cx), (2.0 * x * y + cy)\n",
    "            niter += 1\n",
    "            \n",
    "        julia[tidy, tidx] = niter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "c = cp.complex64(-0.9 + 0.22143j)\n",
    "threads_x = 32\n",
    "threads_y = 32\n",
    "x_points = 5000\n",
    "y_points = 5000\n",
    "x_blocks = (x_points - 1) // threads_x + 1 \n",
    "y_blocks = (y_points - 1) // threads_y + 1\n",
    "X_gpu, Y_gpu = cp.meshgrid(cp.linspace(-2.0 , 2.0, x_points), cp.linspace(-2.0, 2.0, y_points))\n",
    "radius = cp.float64(2.0)\n",
    "with cupy_timer():\n",
    "    julia_gpu = cp.zeros_like(X_gpu, dtype=cp.int32)\n",
    "    julia_kernel[(x_blocks, y_blocks), (threads_x, threads_y)](\n",
    "        X_gpu, Y_gpu, c.real, c.imag, 100, x_points, y_points, radius ** 2, julia_gpu\n",
    "    )\n",
    "    julia_array = julia_gpu.get()\n",
    "    \n",
    "ax.imshow(julia_array, extent=[-2, 2, -2, 2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the source code of the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(julia_kernel.cached_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Exercise</mark>: implement the Mandelbrot set calculation in CuPy using your preferred method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def mandelbrot(X, Y, radius2, itermax):\n",
    "    mandel = np.empty(shape=X.shape, dtype=np.int32)\n",
    "    for i in prange(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            it = 0\n",
    "            cx = X[i, j]\n",
    "            cy = Y[i, j]\n",
    "            x = cx\n",
    "            y = cy\n",
    "            while x * x + y * y < radius2 and it < itermax:\n",
    "                x, y = x * x - y * y + cx, 2.0 * x * y + cy\n",
    "                it += 1\n",
    "            mandel[i, j] = it\n",
    "            \n",
    "    return mandel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(np.linspace(-2.0, 1, 1000), np.linspace(-1.0, 1.0, 1000))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "m = mandelbrot(X, Y, 4.0, 100)\n",
    "    \n",
    "ax.imshow(np.log(1 + m), extent=[-2.0, 1, -1.0, 1.0]);\n",
    "ax.set_aspect('equal')\n",
    "ax.set_ylabel('Im[c]')\n",
    "ax.set_xlabel('Re[c]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_cpu(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "sigmoid_kernel = cp.ElementwiseKernel(\n",
    "    'float64 X',\n",
    "    'float64 sigmoid',\n",
    "    'sigmoid = 1.0 / (1.0 + exp(-X));', \n",
    "    'sigmoid_kernel')\n",
    "\n",
    "@cp.fuse\n",
    "def sigmoid_fused(x):\n",
    "    return 1.0 / (1.0 + cp.exp(-x))\n",
    "\n",
    "def sigmoid_gpu(x):\n",
    "    return 1.0 / (1.0 + cp.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = np.linspace(-10.0, 10.0, 10_000_000)\n",
    "x_gpu = cp.array(x_cpu)\n",
    "\n",
    "print('Sigmoid Cpu: ', end='')\n",
    "with cpu_timer():\n",
    "    s_cpu = sigmoid_cpu(x_cpu)\n",
    "    \n",
    "print('Sigmoid Gpu: ', end='')\n",
    "with cupy_timer():\n",
    "    s_gpu = sigmoid_gpu(x_gpu)\n",
    "    \n",
    "print('Sigmoid Kernel: ', end='')\n",
    "with cupy_timer():\n",
    "    s_gpu_kernel = sigmoid_kernel(x_gpu)\n",
    "    \n",
    "print('Sigmoid Fused: ', end='')\n",
    "with cupy_timer():\n",
    "    s_gpu_fused = sigmoid_fused(x_gpu)\n",
    "    \n",
    "assert(np.allclose(s_cpu, s_gpu.get()))\n",
    "assert(cp.allclose(s_cpu, s_gpu_kernel))\n",
    "assert(cp.allclose(s_cpu, s_gpu_fused))\n",
    "\n",
    "plt.close('all')\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_cpu, s_cpu, '--');\n",
    "ax.set_title('Sigmoid function', )\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y(x)')\n",
    "ax.grid('Both');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CuPy Memory Pools \n",
    "\n",
    "In order to improve performance, CuPy uses a memory pool for memory allocations by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%whos ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = cp.ones((10000, 100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mempool = cp.get_default_memory_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_gbs = mempool.used_bytes() / 1024 ** 3\n",
    "total_gbs = mempool.total_bytes() / 1024 ** 3\n",
    "print(f'Memory pool uses: {used_gbs:.3f} out of {total_gbs:.3f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mempool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cp.cuda.set_allocator(None) # You can disable CuPy memory pool with this "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
